[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joshua Simmons",
    "section": "",
    "text": "welcome_message |&gt; print()\n\n[1] \"Good day\"\n\nabout_message |&gt; print()\n\n[1] \"This is just a little site to hold my posts.\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting a Bayesian linear regression for sandy beach storm response\n\n\n\n\n\n\nPython\n\n\nBayesian\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\nJoshua Simmons\n\n\n\n\n\n\n\n\n\n\n\n\nStreamflow decomposition with mgcv\n\n\n\n\n\n\nR\n\n\nGAM\n\n\n\n\n\n\n\n\n\nJul 4, 2024\n\n\nJoshua Simmons\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/mgcv/01_timeseries_with_mgcv.html",
    "href": "posts/mgcv/01_timeseries_with_mgcv.html",
    "title": "Streamflow decomposition with mgcv",
    "section": "",
    "text": "In this notebook we will explore the use of Generalised Additive Models (GAMs) for timeseries data using the mgcv package. There are many great resources on GAMs (see the ‚ÄúFurther reading‚Äù section for a start), the idea of this notebook is to really guide readers towards these more comprehensive works. To this end we will show the power of GAMs and some of the potential pitfalls (in particular when it comes to timeseries data) in an example that will hopefully resonate more for those with a background in water.\nTo make things easier we will be using synthetic data which will allow us to explore the GAM fitting in a controlled way. We will be using a synthetic streamflow dataset which we aim to disentangle into the various components which contribute the overall signal.\nAs a strand to follow through the analysis, we will set the aim of find the underlying time trend that is present beyond the influence that can be accounted for between the other climatic variables."
  },
  {
    "objectID": "posts/mgcv/01_timeseries_with_mgcv.html#explore-the-data",
    "href": "posts/mgcv/01_timeseries_with_mgcv.html#explore-the-data",
    "title": "Streamflow decomposition with mgcv",
    "section": "2.1 Explore the data",
    "text": "2.1 Explore the data\nLets plot the variables (both the target and the covariates) that we have available to us (all monthly):\n\nrainfall: mm monthly sum\nSOI: Southern Oscillation Index\nmoy: month of the year (1-12 before scaling)\ntime: months from start of the series\nflow: monthly mean streamflow (target)\nlog_flow: log of target\n\nYou can see that the target variable flow is given in ML, however the covariates have all been scaled to be between 0 and 1. Lets plot these up and see what we are working with.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur streamflow is clearly correlated to rainfall as one might expect however it is unclear the influence of broader climatic variability (in this case represented by SOI) or if there are any other persistent trends in the data beyond the effect of SOI."
  },
  {
    "objectID": "posts/mgcv/01_timeseries_with_mgcv.html#here-comes-the-rain",
    "href": "posts/mgcv/01_timeseries_with_mgcv.html#here-comes-the-rain",
    "title": "Streamflow decomposition with mgcv",
    "section": "4.1 Here comes the rain",
    "text": "4.1 Here comes the rain\nWe still haven‚Äôt explored what we could strongly suspect to be the main driver of streamflow variability: rainfall-runoff processes. We expect this to have some nonlinearity, clearly we are grossly simplifying the processes in the catchment, in particular with respect to groundwater interactions. Of course to let you in on a secret, in this example we have employed nonlinearity for this term when creating our synthetic data. So we add a smooth on monthly rainfall (s(rain)).\nIn addition we add in the impact of longer term climatic variability on the streamflow. In this dataset we capture this through the Southern Oscillation Index (SOI) which we interpret as the effect of longer periods of wet and dry on the catchment which contribute to the overall streamflow signal. For example, we might expect say soil moisture to be lower after extended dry periods leading to less streamflow for a given amount of rainfall compared to times with waterlogged soil. Here we introduce another smooth on this term.\n\nm4 &lt;- gam(\n    log_flow ~ s(time) + s(moy, bs=\"cc\") + s(rain) + s(SOI),\n    data = streamflow_data,\n    family = gaussian(link=\"identity\")\n)\nsummary(m4)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nlog_flow ~ s(time) + s(moy, bs = \"cc\") + s(rain) + s(SOI)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.13651    0.01954   58.15   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n          edf Ref.df      F p-value    \ns(time) 3.151  3.905   3.56 0.00723 ** \ns(moy)  6.570  8.000  30.33 &lt; 2e-16 ***\ns(rain) 3.160  3.973 592.26 &lt; 2e-16 ***\ns(SOI)  3.034  3.832  97.17 &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.881   Deviance explained = 88.5%\nGCV = 0.19462  Scale est. = 0.18793   n = 492\n\nappraise(m4)\n\n\n\n\n\n\n\ndraw(m4)\n\n\n\n\n\n\n\npred_plot &lt;- plot_model_prediction(m4, streamflow_data, exp_bool = T)\nwrap_plots(\n    pred_plot,\n    pred_plot + scale_y_log10(),\n    ncol = 1\n)\n\n\n\n\n\n\n\nwrap_plots(\n    pred_plot,\n    draw(derivatives(m4, select=\"s(time)\"), add_change = TRUE, change_type = \"sizer\"),\n    ncol = 1\n)\n\n\n\n\n\n\n\n\nWe have a really good fit now, we are explaining a lot of the variance (R-sq.(adj) = 0.881). We have some interesting and realistic looking smooths on the new variables of rainfall and SOI with low uncertainty. The SOI smooth looks reasonable, during periods of high SOI (La Ni√±a) we might expect wetter conditions compared to periods of low SOI (El Ni√±o). The smooths are all determined to be significant by mgcv but note our time smooth has changed from m3, though again with p=0.00723.\nBut all is not well with our time smooth. We use the gam.check() diagnostic to check our model. Most of the plots will be familiar from our previous dalliance with the appriase() function. However, we also have additional information on the complexity of the smooths. Briefly, by increasing the number of knots available for a spline to fit data, we can increase the ability of the function to take on very complex shapes. The basis dimension checking (see the returned text) is returning that something is off about the s(time) smooth. It‚Äôs indicating that we may need to increase the number of maximum knots in the smooth above the default k=10 to fully explore the space of possible function fits. You can see ?choose.k for more information on this diagnostic and we will explore the implications of this below.\n\ngam.check(m4)\n\n\n\n\n\n\n\n\n\nMethod: GCV   Optimizer: magic\nSmoothing parameter selection converged after 6 iterations.\nThe RMS GCV score gradient at convergence was 8.345812e-07 .\nThe Hessian was positive definite.\nModel rank =  36 / 36 \n\nBasis dimension (k) checking results. Low p-value (k-index&lt;1) may\nindicate that k is too low, especially if edf is close to k'.\n\n          k'  edf k-index p-value    \ns(time) 9.00 3.15    0.23  &lt;2e-16 ***\ns(moy)  8.00 6.57    1.04    0.80    \ns(rain) 9.00 3.16    0.95    0.14    \ns(SOI)  9.00 3.03    0.97    0.21    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "posts/blr/stormerosion_bayesian_linear_regression.html",
    "href": "posts/blr/stormerosion_bayesian_linear_regression.html",
    "title": "Fitting a Bayesian linear regression for sandy beach storm response",
    "section": "",
    "text": "Good old fashioned, handful of parameters, linear regressions are great! Don‚Äôt get me wrong I love fitting larger machine learning (ML) models and when you‚Äôre after raw predictive power there‚Äôs nothing better than finally getting a neural network singing. But in so many applications where you would employ data-driven models, linear regression models (and variations thereof) can hold their own in performance and often be better suited to the project as a whole (manageable, quicker, more interpretable).\nThere‚Äôs something endearing about being able to see all the parameters in your model. A tight knit group that you can call friends when you push that final project deliverable out the door. As ML models get larger, they‚Äôre are at best filled with acquaintances. More often though, you‚Äôre left anxiously gazing into the black-box abyss just hoping to recognise a friendly face.\nFor a while now my default method for fitting linear regressions has been Bayesian. Sure uncertainty quantification is üî•, but really it‚Äôs the flexibility afforded by a Bayesian framework to extend upon simple models to provide just the right level of complexity and to give you a hand in guiding the model to a fit. Actually this post is a vehicle to a future post which will dive into this flexibility and show off some more interesting aspects (hint: it‚Äôll be plenty hierarchical). But we have to get on the bus somewhere, and this is the stop closest to home. Plus I hope there‚Äôll be some nice countryside out the window on the way.\n\n\n\n\n\n\n\n\n\nIs\n\n\n\nA very practical, implementation based introduction to Bayesian methods. A (hopefully) relatable hook to encourage the interested reader into the deeper subject.\n\n\n\n\n\n\n\n\nIs not\n\n\n\nA comprehensive guide to the theory or practice of Bayesian methods. I‚Äôll try refer to other books and resources to point you in the direction of experts.\n\n\n\n\n\n\n\n\n\nAn example of a storm event at Narrabeen Beach (included in the dataset we use)\n\n\nWe are going to use a great, openly available dataset that describes sandy beach response to offshore storms. This is measured by the distance of shoreline change, i.e., in the case of erosion how much the width of the sandy beach is reduced. This comes from the excellent paper:\nData-driven modelling of coastal storm erosion for real-time forecasting at a wave-dominated embayed beach (Ibaceta and Harley, 2024) https://doi.org/10.1016/j.coastaleng.2024.104596\nIn this paper, the authors fit a really nice linear regression to model the storm response. We show here how an equivalent model could be fit with Bayesian methods to show this alternative approach and its concepts.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis is not in any way meant to be knock on the use of a frequentist approach in the above paper. I‚Äôll show some of the differences as we go along of course, but the true power of applying Bayesian methods only comes when fitting more complex models which we will see in the next post. I use the model structure from this paper as the data are freely available, the model is already prepared and has been carefully thought through, and it was an excellent (and clear/well-written) use of data-driven modelling to explore storm erosion!\n\n\n\nAnyway I hear you, ‚Äúpipe down and show me some data‚Äù.\n\n\n\nWe load the data directly from the authors‚Äô github repository (thanks again!). As per the paper, we will attempt to model the change in shoreline position at a given location (dW) with the following predictor variables (which I may interchangeably call covariates, you‚Äôve been warned):\n\nWpre: the shoreline position before the storm\nEocum: cumulative offshore wave energy during the storm (proportional to significant wave height squared)\nWLres: residual of measured water level in relation to the astronomical tide component\nTpeak: peak offshore wave period during the storm\nDpo: average wave direction during the storm\n\nThe raw data:\n\n\n\n\n\n\n\n\n\nOnset\nEnd\ntimezone\nEocum\nDpo\nTpeak\nWLres\nWpre_exposed\nWpre_partially\nWpre_sheltered\ndW_exposed\ndW_partially\ndW_sheltered\n\n\n\n\n90\n31/10/1998 19:00\n3/11/1998 3:00\nAEST\n426251.85140\n154.431507\n12.50\n0.090107\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n91\n18/11/1998 5:00\n19/11/1998 7:00\nAEST\n189300.07370\n141.759592\n10.00\n-0.049956\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n92\n5/02/1999 11:00\n5/02/1999 21:00\nAEST\n56780.11139\n72.999917\n13.33\n-0.135135\n65.97566\n43.044657\n36.475053\n0.0\n0.0\n0.0\n\n\n93\n8/02/1999 8:00\n9/02/1999 4:00\nAEST\n140559.89880\n131.355356\n10.20\n-0.015254\n65.97566\n43.044657\n36.475053\nNaN\nNaN\nNaN\n\n\n94\n22/03/1999 23:00\n23/03/1999 6:00\nAEST\n40664.83559\n158.372335\n10.20\n-0.084564\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nNow there‚Äôs a bit of data wrangling to come in order to get us to the model-ready format you‚Äôll see below. If you wish to see the code I‚Äôve hacked together to get us into a better format for the Bayesian modelling, then feel free to look below. But I wont subject the reader to this by default.\n\n\n\n\n\n\nShow me the code\n\n\n\n\n\nAlright you sick puppy, welcome to the data wrangling.\n\n# Define the target variables\ntarget_variable_names = [ \"dW_exposed\", \"dW_partially\", \"dW_sheltered\"]\n# now our covariates and Onset as a sanity check\ncovariate_vars = [\"Onset\", \"Eocum\", \"WLres\", \"Tpeak\", \"Dpo\"]\n# and the pre-storm shoreline positions\nprestorm_vars = [\"Wpre{}\".format(_[2:]) for _ in target_variable_names]\n\nWe are going to make a little format adjustment just to make filtering between locations easier. In this dataset there are three separate locations (along Narrabeen Beach) at which shoreline change was measured for each storm (named here as ‚Äúexposed‚Äù, ‚Äúpartially‚Äù, and ‚Äúsheltered‚Äù). We are not going to dwell on the differences (you can read the paper if you like), but at a base level we expect the storm response to be slightly different at each of these locations. These are provided as separate columns in the original data, and here we rework the dataframe to have only a single target variable (dW) and convert the locations into a single column as a categorical variable.\n\n# first take only the columns we need\ndf = raw_data[covariate_vars+prestorm_vars+target_variable_names].copy()\n# convert Onset to datetime\ndf[\"Onset\"] = pd.to_datetime(df[\"Onset\"], dayfirst = True)\n# we will convert the separate dW location columns into a dW column\n# and a location column\ndf = pd.melt(\n    df,\n    id_vars = covariate_vars + prestorm_vars,\n    var_name = \"location\",\n    value_name = \"dW\"\n).reset_index(drop=True)\n# lets just remove all the unnecessary dW_ in the variable column\ndf[\"location\"] = df[\"location\"].str.replace(\"dW_\",\"\")\n# we're going similarly grab the correct pre-storm beach position \n# based on the category, the lazy way\nfor ii in np.arange(df.shape[0]):\n    df.loc[ii, \"Wpre\"] = df.loc[ii, \"Wpre_{}\".format(df.loc[ii,\"location\"])]\n# and then drop the now obsolete Wpre columns\ndf = df.drop(columns = prestorm_vars)\n\ndf.sort_values(by = [\"Onset\", \"location\"], inplace = True)\n# and lets get rid of any NaNs\ndf =  df.dropna().reset_index(drop = True)\n\ndisplay(df.head())\n\n\n\n\n\n\n\n\nOnset\nEocum\nWLres\nTpeak\nDpo\nlocation\ndW\nWpre\n\n\n\n\n0\n1999-02-05 11:00:00\n56780.11139\n-0.135135\n13.33\n72.999917\nexposed\n0.0\n65.975660\n\n\n1\n1999-02-05 11:00:00\n56780.11139\n-0.135135\n13.33\n72.999917\npartially\n0.0\n43.044657\n\n\n2\n1999-02-05 11:00:00\n56780.11139\n-0.135135\n13.33\n72.999917\nsheltered\n0.0\n36.475053\n\n\n3\n2003-08-10 02:00:00\n135390.98440\n0.032955\n12.20\n169.821682\nexposed\n0.0\n43.741044\n\n\n4\n2003-08-10 02:00:00\n135390.98440\n0.032955\n12.20\n169.821682\npartially\n0.0\n45.474564\n\n\n\n\n\n\n\nYou should be able to see (e.g., from the Wpre column) the 1999-02-05 storm (row 92 in raw_data) correctly flowing from raw_data through to df below (rows 0 - 2, i.e., three total as there are three locations exposed, partially and sheltered).\nWe should take this time to scale our covariates, the authors did too. Of course we could skip this step but you can see that e.g., Eocum is orders of magnitude higher than WLres. Leaving the data in this state would make it really hard to interpret the coefficients of our model relative to each other for a start. Secondly, our model fitting relies on gradients, and these can get whacky if your variables range over orders of magnitude. Be kind. Show your model some TLC and it‚Äôll make your life easier in return (and if you don‚Äôt believe me, see the ‚ÄúStandardising predictors‚Äù section here).\n\n# find the max and min of each column - we will normalise\n# as the authors did, per location\nscale_max_vals = df.groupby(\"location\").max()\nscale_min_vals = df.groupby(\"location\").min()\n# finally, we normalise each covariate to be between 0 and 1 \n# based on location\nfor col in df.columns.drop([\"Onset\", \"location\"]):\n    # add the location specific values as temporary columns\n    df[\"scale_max\"] = df[\"location\"].map(scale_max_vals[col])\n    df[\"scale_min\"] = df[\"location\"].map(scale_min_vals[col])\n    # scale the variable\n    df[col] = (df[col] - df[\"scale_min\"]) / (df[\"scale_max\"] - df[\"scale_min\"])\n    # hide the evidence\n    df = df.drop(columns = [\"scale_max\", \"scale_min\"])\n\n\n\n\nA few moments later‚Ä¶ we have our data standardised and in the format we want:\n\n\n\n\n\n\n\n\n\nOnset\nEocum\nWLres\nTpeak\nDpo\nlocation\ndW\nWpre\n\n\n\n\n0\n1999-02-05 11:00:00\n0.013590\n0.000000\n0.620448\n0.087123\nexposed\n0.241472\n0.650606\n\n\n1\n1999-02-05 11:00:00\n0.013099\n0.000000\n0.518735\n0.037655\npartially\n0.131938\n0.309427\n\n\n2\n1999-02-05 11:00:00\n0.013099\n0.000000\n0.518735\n0.037655\nsheltered\n0.212869\n0.343740\n\n\n3\n2003-08-10 02:00:00\n0.064453\n0.256852\n0.462185\n0.836539\nexposed\n0.241472\n0.289884\n\n\n4\n2003-08-10 02:00:00\n0.063987\n0.283809\n0.386417\n0.827681\npartially\n0.131938\n0.349153\n\n\n\n\n\n\n\nThe only conceptually important part of this wrangling was the shift to having the location as a categorical variable. Instead of having separate (e.g.,) dW columns for each location, we have a single target variable dW and a location column that tells us which location the data point is from (so we will now have multiple rows for the same storm event). I am signalling here our modelling intent which you will see come into play below and will put us in the right data frame of mind for more complex modelling.\n\n\n\nIn the sections below we will focus on the basics of implementing a Bayesian regression. We‚Äôre limited, of course, by the amount of words that I can responsibly subject you to. So for anyone wanting a comprehensive resource that introduces linear regression from the very basics - Regression and Other Stories by Gelman, Hill and Vehtari.\nTo the model. Again, we are not thinking much about coastal processes here. Luckily, the authors have done the hard work and we just get to sit back and crunch numbers üòé. So we follow the model provided in the paper:\n\\[\\Delta W = \\beta_0 + \\beta_1 E_{o,cum} + \\beta_2 W_{pre} + \\beta_3 D_{po} + \\beta_4 T_{p,peak} + \\beta_5 WL_{res} + \\epsilon\\]\nWe have a model that predicts the change in shoreline (\\(\\Delta W\\)) from the additive combination of our five variables (along with an intercept term \\(\\beta_0\\)).\nTo help us intuit the Bayesian approach to fitting, lets simplify our model. We label our five variables plus the constant to represent the intercept collectively as \\(\\mathbf{X}\\). The corresponding coefficients that we must fit for each we label collectively as \\(\\mathbf{\\beta}\\) (see the here if this notation is unfamiliar).\nWe can condense our model to:\n\\[\n\\Delta W = \\mathbf{X} \\mathbf{\\beta} + \\epsilon\n\\]\nBut we mustn‚Äôt forget to discuss the little red caboose languishing at the end, \\(\\epsilon\\). Sometimes it doesn‚Äôt get the attention of the cars up front, but it‚Äôs a vital part of the train in any gradient ascent. One way to parse the above would be:\n\n\\(\\mathbf{X} \\mathbf{\\beta}\\) describes our model estimating the mean values of \\(\\Delta W\\)\n\\(\\epsilon\\) describes the residuals or errors of our model when considering our data\n\nIn this case we are going to assume that our residuals are normally distributed with a standard deviation of \\(\\sigma\\) and centred around zero (\\(\\mathcal{N}(0, \\sigma)\\)). Why this distribution? This is a good start and a common assumption. Lot of things in nature are gaussian. Plus thinking about the alternatives too hard would lead us into the scary world of generalised linear models, and that‚Äôs not for us today.\nOkay so putting words into notation:\n\\[\n\\Delta W = \\mathbf{X} \\mathbf{\\beta} + \\epsilon \\qquad \\epsilon \\sim \\mathcal{N}(0, \\sigma)\n\\]\nWe could also frame this in another way that is perhaps more Bayesian and will align better with our code below. We could say that our observed values of \\(\\Delta W\\) are generated from a normal distribution with mean \\(\\mathbf{X} \\mathbf{\\beta}\\) and standard deviation \\(\\sigma\\). Same emperor, new clothes. \\(\\sigma\\) then becomes another parameter we will learn from the data.\n\\[\n\\Delta W \\sim \\mathcal{N}(\\mathbf{X} \\mathbf{\\beta}, \\sigma)\n\\]"
  },
  {
    "objectID": "posts/blr/stormerosion_bayesian_linear_regression.html#an-ode-to-linear-regressions",
    "href": "posts/blr/stormerosion_bayesian_linear_regression.html#an-ode-to-linear-regressions",
    "title": "Fitting a Bayesian linear regression for sandy beach storm response",
    "section": "",
    "text": "Good old fashioned, handful of parameters, linear regressions are great! Don‚Äôt get me wrong I love fitting larger machine learning (ML) models and when you‚Äôre after raw predictive power there‚Äôs nothing better than finally getting a neural network singing. But in so many applications where you would employ data-driven models, linear regression models (and variations thereof) can hold their own in performance and often be better suited to the project as a whole (manageable, quicker, more interpretable).\nThere‚Äôs something endearing about being able to see all the parameters in your model. A tight knit group that you can call friends when you push that final project deliverable out the door. As ML models get larger, they‚Äôre are at best filled with acquaintances. More often though, you‚Äôre left anxiously gazing into the black-box abyss just hoping to recognise a friendly face.\nFor a while now my default method for fitting linear regressions has been Bayesian. Sure uncertainty quantification is üî•, but really it‚Äôs the flexibility afforded by a Bayesian framework to extend upon simple models to provide just the right level of complexity and to give you a hand in guiding the model to a fit. Actually this post is a vehicle to a future post which will dive into this flexibility and show off some more interesting aspects (hint: it‚Äôll be plenty hierarchical). But we have to get on the bus somewhere, and this is the stop closest to home. Plus I hope there‚Äôll be some nice countryside out the window on the way."
  },
  {
    "objectID": "posts/blr/stormerosion_bayesian_linear_regression.html#what-this-is-and-what-this-is-not",
    "href": "posts/blr/stormerosion_bayesian_linear_regression.html#what-this-is-and-what-this-is-not",
    "title": "Fitting a Bayesian linear regression for sandy beach storm response",
    "section": "",
    "text": "Is\n\n\n\nA very practical, implementation based introduction to Bayesian methods. A (hopefully) relatable hook to encourage the interested reader into the deeper subject.\n\n\n\n\n\n\n\n\nIs not\n\n\n\nA comprehensive guide to the theory or practice of Bayesian methods. I‚Äôll try refer to other books and resources to point you in the direction of experts."
  },
  {
    "objectID": "posts/blr/stormerosion_bayesian_linear_regression.html#the-task",
    "href": "posts/blr/stormerosion_bayesian_linear_regression.html#the-task",
    "title": "Fitting a Bayesian linear regression for sandy beach storm response",
    "section": "",
    "text": "An example of a storm event at Narrabeen Beach (included in the dataset we use)\n\n\nWe are going to use a great, openly available dataset that describes sandy beach response to offshore storms. This is measured by the distance of shoreline change, i.e., in the case of erosion how much the width of the sandy beach is reduced. This comes from the excellent paper:\nData-driven modelling of coastal storm erosion for real-time forecasting at a wave-dominated embayed beach (Ibaceta and Harley, 2024) https://doi.org/10.1016/j.coastaleng.2024.104596\nIn this paper, the authors fit a really nice linear regression to model the storm response. We show here how an equivalent model could be fit with Bayesian methods to show this alternative approach and its concepts.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis is not in any way meant to be knock on the use of a frequentist approach in the above paper. I‚Äôll show some of the differences as we go along of course, but the true power of applying Bayesian methods only comes when fitting more complex models which we will see in the next post. I use the model structure from this paper as the data are freely available, the model is already prepared and has been carefully thought through, and it was an excellent (and clear/well-written) use of data-driven modelling to explore storm erosion!\n\n\n\nAnyway I hear you, ‚Äúpipe down and show me some data‚Äù."
  },
  {
    "objectID": "posts/blr/stormerosion_bayesian_linear_regression.html#the-data",
    "href": "posts/blr/stormerosion_bayesian_linear_regression.html#the-data",
    "title": "Fitting a Bayesian linear regression for sandy beach storm response",
    "section": "",
    "text": "We load the data directly from the authors‚Äô github repository (thanks again!). As per the paper, we will attempt to model the change in shoreline position at a given location (dW) with the following predictor variables (which I may interchangeably call covariates, you‚Äôve been warned):\n\nWpre: the shoreline position before the storm\nEocum: cumulative offshore wave energy during the storm (proportional to significant wave height squared)\nWLres: residual of measured water level in relation to the astronomical tide component\nTpeak: peak offshore wave period during the storm\nDpo: average wave direction during the storm\n\nThe raw data:\n\n\n\n\n\n\n\n\n\nOnset\nEnd\ntimezone\nEocum\nDpo\nTpeak\nWLres\nWpre_exposed\nWpre_partially\nWpre_sheltered\ndW_exposed\ndW_partially\ndW_sheltered\n\n\n\n\n90\n31/10/1998 19:00\n3/11/1998 3:00\nAEST\n426251.85140\n154.431507\n12.50\n0.090107\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n91\n18/11/1998 5:00\n19/11/1998 7:00\nAEST\n189300.07370\n141.759592\n10.00\n-0.049956\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n92\n5/02/1999 11:00\n5/02/1999 21:00\nAEST\n56780.11139\n72.999917\n13.33\n-0.135135\n65.97566\n43.044657\n36.475053\n0.0\n0.0\n0.0\n\n\n93\n8/02/1999 8:00\n9/02/1999 4:00\nAEST\n140559.89880\n131.355356\n10.20\n-0.015254\n65.97566\n43.044657\n36.475053\nNaN\nNaN\nNaN\n\n\n94\n22/03/1999 23:00\n23/03/1999 6:00\nAEST\n40664.83559\n158.372335\n10.20\n-0.084564\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nNow there‚Äôs a bit of data wrangling to come in order to get us to the model-ready format you‚Äôll see below. If you wish to see the code I‚Äôve hacked together to get us into a better format for the Bayesian modelling, then feel free to look below. But I wont subject the reader to this by default.\n\n\n\n\n\n\nShow me the code\n\n\n\n\n\nAlright you sick puppy, welcome to the data wrangling.\n\n# Define the target variables\ntarget_variable_names = [ \"dW_exposed\", \"dW_partially\", \"dW_sheltered\"]\n# now our covariates and Onset as a sanity check\ncovariate_vars = [\"Onset\", \"Eocum\", \"WLres\", \"Tpeak\", \"Dpo\"]\n# and the pre-storm shoreline positions\nprestorm_vars = [\"Wpre{}\".format(_[2:]) for _ in target_variable_names]\n\nWe are going to make a little format adjustment just to make filtering between locations easier. In this dataset there are three separate locations (along Narrabeen Beach) at which shoreline change was measured for each storm (named here as ‚Äúexposed‚Äù, ‚Äúpartially‚Äù, and ‚Äúsheltered‚Äù). We are not going to dwell on the differences (you can read the paper if you like), but at a base level we expect the storm response to be slightly different at each of these locations. These are provided as separate columns in the original data, and here we rework the dataframe to have only a single target variable (dW) and convert the locations into a single column as a categorical variable.\n\n# first take only the columns we need\ndf = raw_data[covariate_vars+prestorm_vars+target_variable_names].copy()\n# convert Onset to datetime\ndf[\"Onset\"] = pd.to_datetime(df[\"Onset\"], dayfirst = True)\n# we will convert the separate dW location columns into a dW column\n# and a location column\ndf = pd.melt(\n    df,\n    id_vars = covariate_vars + prestorm_vars,\n    var_name = \"location\",\n    value_name = \"dW\"\n).reset_index(drop=True)\n# lets just remove all the unnecessary dW_ in the variable column\ndf[\"location\"] = df[\"location\"].str.replace(\"dW_\",\"\")\n# we're going similarly grab the correct pre-storm beach position \n# based on the category, the lazy way\nfor ii in np.arange(df.shape[0]):\n    df.loc[ii, \"Wpre\"] = df.loc[ii, \"Wpre_{}\".format(df.loc[ii,\"location\"])]\n# and then drop the now obsolete Wpre columns\ndf = df.drop(columns = prestorm_vars)\n\ndf.sort_values(by = [\"Onset\", \"location\"], inplace = True)\n# and lets get rid of any NaNs\ndf =  df.dropna().reset_index(drop = True)\n\ndisplay(df.head())\n\n\n\n\n\n\n\n\nOnset\nEocum\nWLres\nTpeak\nDpo\nlocation\ndW\nWpre\n\n\n\n\n0\n1999-02-05 11:00:00\n56780.11139\n-0.135135\n13.33\n72.999917\nexposed\n0.0\n65.975660\n\n\n1\n1999-02-05 11:00:00\n56780.11139\n-0.135135\n13.33\n72.999917\npartially\n0.0\n43.044657\n\n\n2\n1999-02-05 11:00:00\n56780.11139\n-0.135135\n13.33\n72.999917\nsheltered\n0.0\n36.475053\n\n\n3\n2003-08-10 02:00:00\n135390.98440\n0.032955\n12.20\n169.821682\nexposed\n0.0\n43.741044\n\n\n4\n2003-08-10 02:00:00\n135390.98440\n0.032955\n12.20\n169.821682\npartially\n0.0\n45.474564\n\n\n\n\n\n\n\nYou should be able to see (e.g., from the Wpre column) the 1999-02-05 storm (row 92 in raw_data) correctly flowing from raw_data through to df below (rows 0 - 2, i.e., three total as there are three locations exposed, partially and sheltered).\nWe should take this time to scale our covariates, the authors did too. Of course we could skip this step but you can see that e.g., Eocum is orders of magnitude higher than WLres. Leaving the data in this state would make it really hard to interpret the coefficients of our model relative to each other for a start. Secondly, our model fitting relies on gradients, and these can get whacky if your variables range over orders of magnitude. Be kind. Show your model some TLC and it‚Äôll make your life easier in return (and if you don‚Äôt believe me, see the ‚ÄúStandardising predictors‚Äù section here).\n\n# find the max and min of each column - we will normalise\n# as the authors did, per location\nscale_max_vals = df.groupby(\"location\").max()\nscale_min_vals = df.groupby(\"location\").min()\n# finally, we normalise each covariate to be between 0 and 1 \n# based on location\nfor col in df.columns.drop([\"Onset\", \"location\"]):\n    # add the location specific values as temporary columns\n    df[\"scale_max\"] = df[\"location\"].map(scale_max_vals[col])\n    df[\"scale_min\"] = df[\"location\"].map(scale_min_vals[col])\n    # scale the variable\n    df[col] = (df[col] - df[\"scale_min\"]) / (df[\"scale_max\"] - df[\"scale_min\"])\n    # hide the evidence\n    df = df.drop(columns = [\"scale_max\", \"scale_min\"])\n\n\n\n\nA few moments later‚Ä¶ we have our data standardised and in the format we want:\n\n\n\n\n\n\n\n\n\nOnset\nEocum\nWLres\nTpeak\nDpo\nlocation\ndW\nWpre\n\n\n\n\n0\n1999-02-05 11:00:00\n0.013590\n0.000000\n0.620448\n0.087123\nexposed\n0.241472\n0.650606\n\n\n1\n1999-02-05 11:00:00\n0.013099\n0.000000\n0.518735\n0.037655\npartially\n0.131938\n0.309427\n\n\n2\n1999-02-05 11:00:00\n0.013099\n0.000000\n0.518735\n0.037655\nsheltered\n0.212869\n0.343740\n\n\n3\n2003-08-10 02:00:00\n0.064453\n0.256852\n0.462185\n0.836539\nexposed\n0.241472\n0.289884\n\n\n4\n2003-08-10 02:00:00\n0.063987\n0.283809\n0.386417\n0.827681\npartially\n0.131938\n0.349153\n\n\n\n\n\n\n\nThe only conceptually important part of this wrangling was the shift to having the location as a categorical variable. Instead of having separate (e.g.,) dW columns for each location, we have a single target variable dW and a location column that tells us which location the data point is from (so we will now have multiple rows for the same storm event). I am signalling here our modelling intent which you will see come into play below and will put us in the right data frame of mind for more complex modelling."
  },
  {
    "objectID": "posts/blr/stormerosion_bayesian_linear_regression.html#the-model",
    "href": "posts/blr/stormerosion_bayesian_linear_regression.html#the-model",
    "title": "Fitting a Bayesian linear regression for sandy beach storm response",
    "section": "",
    "text": "In the sections below we will focus on the basics of implementing a Bayesian regression. We‚Äôre limited, of course, by the amount of words that I can responsibly subject you to. So for anyone wanting a comprehensive resource that introduces linear regression from the very basics - Regression and Other Stories by Gelman, Hill and Vehtari.\nTo the model. Again, we are not thinking much about coastal processes here. Luckily, the authors have done the hard work and we just get to sit back and crunch numbers üòé. So we follow the model provided in the paper:\n\\[\\Delta W = \\beta_0 + \\beta_1 E_{o,cum} + \\beta_2 W_{pre} + \\beta_3 D_{po} + \\beta_4 T_{p,peak} + \\beta_5 WL_{res} + \\epsilon\\]\nWe have a model that predicts the change in shoreline (\\(\\Delta W\\)) from the additive combination of our five variables (along with an intercept term \\(\\beta_0\\)).\nTo help us intuit the Bayesian approach to fitting, lets simplify our model. We label our five variables plus the constant to represent the intercept collectively as \\(\\mathbf{X}\\). The corresponding coefficients that we must fit for each we label collectively as \\(\\mathbf{\\beta}\\) (see the here if this notation is unfamiliar).\nWe can condense our model to:\n\\[\n\\Delta W = \\mathbf{X} \\mathbf{\\beta} + \\epsilon\n\\]\nBut we mustn‚Äôt forget to discuss the little red caboose languishing at the end, \\(\\epsilon\\). Sometimes it doesn‚Äôt get the attention of the cars up front, but it‚Äôs a vital part of the train in any gradient ascent. One way to parse the above would be:\n\n\\(\\mathbf{X} \\mathbf{\\beta}\\) describes our model estimating the mean values of \\(\\Delta W\\)\n\\(\\epsilon\\) describes the residuals or errors of our model when considering our data\n\nIn this case we are going to assume that our residuals are normally distributed with a standard deviation of \\(\\sigma\\) and centred around zero (\\(\\mathcal{N}(0, \\sigma)\\)). Why this distribution? This is a good start and a common assumption. Lot of things in nature are gaussian. Plus thinking about the alternatives too hard would lead us into the scary world of generalised linear models, and that‚Äôs not for us today.\nOkay so putting words into notation:\n\\[\n\\Delta W = \\mathbf{X} \\mathbf{\\beta} + \\epsilon \\qquad \\epsilon \\sim \\mathcal{N}(0, \\sigma)\n\\]\nWe could also frame this in another way that is perhaps more Bayesian and will align better with our code below. We could say that our observed values of \\(\\Delta W\\) are generated from a normal distribution with mean \\(\\mathbf{X} \\mathbf{\\beta}\\) and standard deviation \\(\\sigma\\). Same emperor, new clothes. \\(\\sigma\\) then becomes another parameter we will learn from the data.\n\\[\n\\Delta W \\sim \\mathcal{N}(\\mathbf{X} \\mathbf{\\beta}, \\sigma)\n\\]"
  },
  {
    "objectID": "posts/blr/stormerosion_bayesian_linear_regression.html#raising-a-model",
    "href": "posts/blr/stormerosion_bayesian_linear_regression.html#raising-a-model",
    "title": "Fitting a Bayesian linear regression for sandy beach storm response",
    "section": "2.1 Raising a model",
    "text": "2.1 Raising a model\nWe are going to talk more about prior distributions below. For now all we need to know is that our model needs us to work hand in hand with it. A Bayesian approach is guiding the model, supporting it and empowering it to achieve a good fit. Infinity is an frighteningly large search space, and without some guidance we are sending our model out in the big wide world all alone. We use prior knowledge to define some reasonable space in which we expect to find plausible values for our parameters.\nThe aim is to give the model a nice wide field to explore but not have it wander off a numerical cliff. In environmental science its fairly easy to define what‚Äôs silly. For example, can a shoreline erode 100,000 m during a single storm event? If it could we‚Äôd be in strife so we can pretty safely give very little probability to parameter values that produce such outlandish numbers. We use prior distributions to impart this belief on the model, leaving it then do its own exploration to flesh out the posterior with this information, the data, and the likelihood function."
  },
  {
    "objectID": "posts/blr/stormerosion_bayesian_linear_regression.html#likelihood",
    "href": "posts/blr/stormerosion_bayesian_linear_regression.html#likelihood",
    "title": "Fitting a Bayesian linear regression for sandy beach storm response",
    "section": "2.2 Likelihood",
    "text": "2.2 Likelihood\nThe likelihood is a measure of how likely the data are given the model and a set of parameter values. The better fit to the data that a set of parameter values gives, the more we should believe that those parameter values are plausible. We‚Äôve distilled things down a bit there, as our posterior distribution is influenced by both out prior and likelihood, but that‚Äôs the intuition.\nLuckily for us, the model we specified above and our assumptions about the data generating process in this case readily give rise to a likelihood. And even luckier for us, when we use modern software to specify and fit our model, we can get it without even breaking a sweat.\nWe will use a Probabilistic Programming Language (PPL) of which there are many options including NumPyro, Stan and PyMC. PPLs provide a way to specify Bayesian models, and use algorithms (such as NUTS) to sample and estimate the posterior.\nWe‚Äôre going to look at how this works in a code example below (using NumPyro) which will hopefully help to illuminate this. I am going to generate some synthetic data that relates a variable X to target y with a simple linear relationship plus gaussian noise (np.random.randn).\n\n# make dummy data for X, then generate y data using:\n# y = 0.1 X + 0.2 + N(0, 0.1)\nX = np.random.randn(100)\ny = 0.1 * X + 0.2 + 0.1 * np.random.randn(100)\n\nWe can specify a linear regression model: \\[y = \\mathcal{N}\\left(\\beta_0 + \\beta_1 x, \\sigma\\right)\\]\nusing a PPL and then fit values for \\(\\beta_0\\) and \\(\\beta_1\\) (and \\(\\sigma\\)). Given some proposed values for the parameters, NumPyro will tell us the log-likelihood for these values given the model and the data. Sweet.\n\n\n\n\n\n\nShow me the code\n\n\n\n\n\nWhat you‚Äôll see below is how to specify this type of model in NumPyro. Don‚Äôt get too hung up on this. We will be using the much simpler Bambi below and I‚Äôll leave the proper explanation of NumPyro to a separate post.\n\n# build a simple model for this in NumPyro\ndef model(X, y=None):\n    # Define priors\n1    beta_0 = numpyro.sample(\"beta_0\", dist.Normal(0, 1))\n    beta_1 = numpyro.sample(\"beta_1\", dist.Normal(0, 1))\n    sigma = numpyro.sample(\"sigma\", dist.Exponential(1))\n    # Define the model\n2    mu = beta_0 + beta_1 * X\n    # store the model prediction before we account for the error\n    numpyro.deterministic(\"mu\", mu)\n    # and then finally sample so we can compare to our observations\n3    numpyro.sample(\"y_out\", dist.Normal(mu, sigma), obs = y)\n\n\n1\n\nThe first few lines define the prior distri butions for the parameters in our model - we go into these further in Section 3.\n\n2\n\nWe define the model as a linear regression with an intercept (beta_0) and a slope (beta_1). mu (\\(\\mu\\)) stores the mean prediction of this model as we defined above i.e., \\(\\mu = \\mathbf{X}\\mathbf{\\beta}\\).\n\n3\n\nFinally we tell the model how we think the residuals are distributed about \\(\\mu\\). This line is a direct translation of our equation from above \\(y \\sim \\mathcal{N}(\\mathbf{X} \\mathbf{\\beta}, \\sigma)\\).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe are making the assumption that our \\(y\\) are independent given the covariates. This is often not the case in timeseries where autocorrelation lurks waiting for an opportunity to ruin your inferences and your day, but we won‚Äôt delve deep here.\n\n\n\n\n\nLog-likelihood code\n# generate 10 possible solutions\nproposed_values = {\n    \"beta_0\" : np.array([0.2, 0.1, 0.05, 0.19, 0.21, 0.4, 0.45, 0.25, 0.05, 0.2]),\n    \"beta_1\" : np.array([0.1, 0.2, 0.05, 0.11, 0.09, 0, -0.1, 0.15, 0.2, 0.3]),\n    \"sigma\" : np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n}\n# get the log likelihood\nloglik = numpyro.infer.util.log_likelihood(\n    model = model, \n    posterior_samples = proposed_values,\n    X = X, \n    y = y\n)\n\n\n\n\n\nYou can see below that candidate models with parameter values that lie closer to our true model parameters (\\(0.1 * x + 0.2\\)) have higher log-likelihoods. As we would expect right, they fit the data better. So there we have it, a method for assessing goodness of fit. We‚Äôre well on our way to a posterior with much of the hard work having been done for us, goodee.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor the graph above: the higher the likelihood the better (hence why you may have heard of maximum likelihood estimation)."
  },
  {
    "objectID": "posts/blr/stormerosion_bayesian_linear_regression.html#relax-and-let-the-machine-do-its-job",
    "href": "posts/blr/stormerosion_bayesian_linear_regression.html#relax-and-let-the-machine-do-its-job",
    "title": "Fitting a Bayesian linear regression for sandy beach storm response",
    "section": "2.3 Relax and let the machine do its job",
    "text": "2.3 Relax and let the machine do its job\nOur machine for obtaining samples and building a picture of the posterior distribution for our parameters is going to be Markov Chain Monte Carlo (MCMC). For a great introduction to MCMC please go explore many better sources than this, for example this lecture from Richard McElreath‚Äôs great Statistical Rethinking course.\nFor our purposes though, and for the rest of this blog, we are going to treat the process of obtaining the posterior distribution as magic. We first specify our priors and the data generating process with our model. Then the python implementation of the sampling fairy floats down from the clouds, pries open the chassis of my laptop and greedily feeds on the power being fed to the CPU for a period of seconds to minutes. The gorged fairy, now satiated, waves its magic wand and voil√†! Samples from the posterior distribution are delivered into memory, rendered into figures, and subsequently delivered express to our eyeballs.\nOf course, I should mention that we pay for this approach of representing parameters as distributions instead of a single value. The Bayesian fit will give us flashy uncertainty bands et al.¬†but for a (mostly modest) computational cost. Sometimes this cost can sneak up on you for more complex models, but for a lot of problems in the environmental space the time difference is manageable on modern compute. I‚Äôve skimmed through enough articles to know attention spans are shorter than ever, but surely we can spare a few extra seconds to give our model a glow up."
  },
  {
    "objectID": "posts/blr/stormerosion_bayesian_linear_regression.html#a-location-smoothie",
    "href": "posts/blr/stormerosion_bayesian_linear_regression.html#a-location-smoothie",
    "title": "Fitting a Bayesian linear regression for sandy beach storm response",
    "section": "3.1 A location smoothie",
    "text": "3.1 A location smoothie\nWe‚Äôve now built a little more intuition about how we fit the model in Bayes and estimate the elusive posterior distribution. So let‚Äôs turn back to our original example. We‚Äôre going to look at how to mix the ingredients ready for sampling using the Bambi package. Bambi relies on the PyMC package as the underlying PPL (see Section 2) to construct the model and sample from the posterior distribution.\nFor Bambi to work its magic, we need to:\n\ndescribe the parameters in our model and give prior distributions for each (though Bambi will adopt some default priors if you‚Äôre feeling lucky)\nspecify the model that, given the data and the parameters, estimates the target (y)\n\n\n# define our model - a smoothie mixing all locations into one\nmod_smoothie = bmb.Model(\n1        formula = \"dW ~ 1 + Eocum + Wpre + Dpo + Tpeak + WLres\",\n2        data = df,\n3        priors = {\n            \"Intercept\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n            \"Eocum\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n            \"Wpre\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n            \"Dpo\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n            \"Tpeak\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n            \"WLres\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n            \"sigma\": bmb.Prior(\"HalfNormal\", sigma = 2)\n        },\n4        family = \"gaussian\"\n    )\n\n# tell Bambi to draw samples using MCMC\n5fit_smoothie = mod_smoothie.fit(\n    draws = 2000\n)\n\n# and make predcictions (added to fit_smoothie)\n6mod_smoothie.predict(fit_smoothie, kind = \"response\")\n\n\n1\n\nThis is the model itself. It‚Äôs the same equation we saw above refering to the columns of the data (from the df data frame that we will use). Bambi then infers it must fit a \\(\\beta\\) term per covariate (plus an intercept by default). We have defined how we expect the input to be related to the output (in this case via a simple additive linear model).\n\n2\n\nModels, oh how they covet data.\n\n3\n\nReally the specification of the prior is the main new part from what the authors did in their paper. Here we list each of our parameters (by naming the variables with corresponding \\(\\beta\\) parameters), and tell the model a space in which we expect to find the plausible values for these parameters. Here we won‚Äôt think to hard about this, how about this: we place 68% probability on our paramaeter values being between -1 and 1, 95% between -2 and 2. Ths corresponds to a prior of \\(\\beta \\sim \\mathcal{N}(0, 1)\\). We have one other type of prior we used, and that is the HalfNormal prior for \\(\\sigma\\). \\(\\sigma\\) is a little special compared to the other parameters and that‚Äôs because it cannot be negative. Why? Well simply a normal distribution with negative standard deviation doesn‚Äôt make sense. So in this case we use a half normal prior (bounded at 0) to enforce positivity.\n\n4\n\nThe next part specifies the form of the residuals if you want to think about it that way, or the final part in the data generating process modelling the distribution of observations given the model as we saw above. This defines our likelihood and we make a gaussian assumption as discussed above.\n\n5\n\nNow lets fit the model, letting the sampling fairy do its work to take 2000 samples from the posterior using MCMC.\n\n6\n\nLast of all we make predictions on the data using the model so that we can compare our Bayesian model fit to Figure 8 of the paper and check we haven‚Äôt stuffed anything up.\n\n\n\n\nYou‚Äôre so close now to seeing model output, but following a good Bayesian workflow we‚Äôre going to check our priors first. Remember we set out with the goal of merely shaving the edges off infinity to give a sensible range for the model to consider. Lets wee how we did. Luckily Bambi provides a convenience functions to this end. We plot the priors for each parameter first with .plot_priors(). You can see in practice how much weight we are giving various values, as we said above telling the model we are quite sure the values lie between -2 and 2.\n\n# plot the prior distributions themselves \nmod_smoothie.plot_priors(figsize = (9, 8))\n# make things a little less cosy\nplt.subplots_adjust(hspace = 0.5)\n# and sample from the priors to get predictions\nprior_pred = mod_smoothie.prior_predictive()\n\nSampling: [Dpo, Eocum, Intercept, Tpeak, WLres, Wpre, sigma]\nSampling: [Dpo, Eocum, Intercept, Tpeak, WLres, Wpre, dW, sigma]\n\n\n\n\n\n\n\n\n\nThe second convenience function we used is prior_predictive(). This is kind of cool. Because of our conceptualisation of the model as representing a data generating process we can simulate observations straight off the bat using only our priors. So we can see below that for the top 10 storms, our prior model predicts values of shoreline change will be less than 1000s of meters. Our model predicts a mean of around 0 and the coloured bars show the 89% credible intervals for our predictions (more on this below). So again, we‚Äôre not being too restrictive and we definitely cover all the reasonable parameter values. However, we‚Äôve helped the sampler into a reasonable search space.\n\n\n\n\n\n\n\n\n\nAnd now we come to our beautiful posterior distributions which we can examine using the arviz package. The posterior distributions are shown for each of our variables listed on the y axis. 89% credible intervals are shown, as well as the interquartile range slightly bolder, and the median value with a dot.\n\nax1 = az.plot_forest(\n    fit_smoothie,\n    var_names = [\"~sigma\", \"~mu\"],\n    combined = True,\n    hdi_prob = 0.89,\n    figsize = (5, 4)\n)\n\n\n\n\n\n\n\n\nOkay we have some plausible \\(\\beta\\) values now. You can read the paper if you want the indepth analysis, but for this model for example we see shoreline change is most strongly associated with \\(E_{o,cum}\\), with higher offshore wave energy producing more erosion on average (XX erosion is defined as positive). We can see that we are quite uncertain as to the effect of \\(WL_{res}\\) with the credible intervals straddling zero (note: for this initial model).\nWhile we won‚Äôt cover it here, arviz provides excellent diagnostics for viewing the sampling traces and checking for convergence. This can be really useful if things are going wrong with your model, Bayes will typically be quite transparent when you‚Äôve messed up or where MCMC hasn‚Äôt been able to get a representative picture of the posterior. While this can sometimes lead to hours/days of hair pulling trying to get complex models working, I think that a tendency not to silently fail is a real strength. Good friends have the courage to tell you when you‚Äôve made a mistake.\n\n\n\n\n\n\nShow me example code\n\n\n\n\n\n\n# plot the trace to see how sampling progressed \naz.plot_trace(\n    fit_smoothie,\n    var_names = [\"Intercept\", \"sigma\"],\n    figsize = (9, 5)\n)\n# convergence diagnostics - close to 1 is good\naz.rhat(fit_smoothie)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 10kB\nDimensions:    (__obs__: 597)\nCoordinates:\n  * __obs__    (__obs__) int64 5kB 0 1 2 3 4 5 6 ... 590 591 592 593 594 595 596\nData variables:\n    Dpo        float64 8B 1.0\n    Eocum      float64 8B 1.001\n    Intercept  float64 8B 1.001\n    Tpeak      float64 8B 1.0\n    WLres      float64 8B 1.001\n    Wpre       float64 8B 1.0\n    sigma      float64 8B 1.0\n    mu         (__obs__) float64 5kB 1.001 1.001 1.001 1.0 ... 1.0 1.0 1.0 1.0xarray.DatasetDimensions:__obs__: 597Coordinates: (1)__obs__(__obs__)int640 1 2 3 4 5 ... 592 593 594 595 596array([  0,   1,   2, ..., 594, 595, 596])Data variables: (8)Dpo()float641.0array(1.00007404)Eocum()float641.001array(1.00066998)Intercept()float641.001array(1.00083709)Tpeak()float641.0array(1.0001899)WLres()float641.001array(1.00051769)Wpre()float641.0array(1.00030667)sigma()float641.0array(1.00029748)mu(__obs__)float641.001 1.001 1.001 ... 1.0 1.0 1.0array([1.00092294, 1.00093209, 1.00096025, 1.00011602, 1.00003782,\n       1.0000731 , 1.00006197, 1.00005858, 1.00072858, 1.00074579,\n       1.00065131, 1.00060554, 0.99994075, 0.99995411, 0.99991476,\n       0.99984711, 1.00058798, 1.00062724, 1.00033202, 1.00019305,\n       1.00007434, 1.00009299, 1.00019115, 1.00020769, 1.00068003,\n       1.00066226, 0.99998394, 0.99993679, 0.99995498, 1.00002185,\n       1.00196942, 1.0009044 , 1.00096597, 1.00095372, 1.00012059,\n       1.0005848 , 1.00001719, 1.00008915, 0.99985761, 1.00008344,\n       1.00008671, 1.0000572 , 1.00009536, 1.00005219, 1.00003088,\n       1.00112425, 1.00121702, 1.00042001, 1.00041381, 1.0006448 ,\n       1.00068264, 1.00037434, 1.00054606, 1.00027658, 1.00054001,\n       0.99986065, 0.99999099, 1.00022685, 1.00029698, 1.00083309,\n       1.00083057, 1.00089461, 1.00096924, 1.00191828, 1.00191519,\n       1.00045561, 1.00004991, 1.00037741, 1.00043184, 1.00017006,\n       1.00002153, 0.99993968, 1.00002424, 1.00013145, 1.0006337 ,\n       1.00006675, 1.00011478, 0.99970782, 1.00025873, 1.00022962,\n       1.00028724, 1.00070839, 1.00055585, 1.00047337, 1.00082222,\n       1.00023622, 1.00033297, 1.00006001, 1.00013856, 1.00052354,\n       1.00009673, 1.00010309, 1.00007978, 1.00019873, 1.00000537,\n       1.00013427, 1.00021232, 1.00002887, 1.00095334, 1.00065222,\n...\n       0.99973635, 1.00070978, 1.00048544, 1.00061887, 1.00053148,\n       1.00025148, 1.0006484 , 1.00071295, 1.00039559, 1.00003079,\n       0.99992449, 1.00028824, 0.99982708, 0.99984023, 1.00073178,\n       0.99996067, 0.99988972, 1.00054635, 1.00040201, 1.00037778,\n       1.0004866 , 1.0004464 , 1.0005684 , 1.00020832, 1.00035982,\n       1.00026012, 1.00028584, 1.00078827, 1.00053385, 1.00031214,\n       1.000214  , 1.00004337, 1.00069304, 1.00041599, 1.00049199,\n       1.00020573, 0.99999331, 1.00034491, 1.00046781, 1.00081863,\n       1.00052225, 1.00024028, 1.00048379, 1.00037587, 0.99985622,\n       1.0003253 , 1.00058024, 1.00078568, 1.00071657, 1.00034203,\n       1.00011935, 1.00010904, 1.00009435, 1.00005717, 1.00001568,\n       1.00039145, 1.00031521, 1.00032847, 1.00038992, 1.00057291,\n       1.00143974, 1.00014213, 1.00006258, 1.00046755, 1.00000631,\n       1.00015707, 0.99985738, 1.00006774, 0.99996367, 1.00023056,\n       0.99972864, 0.99994387, 1.00047291, 1.00051394, 1.00066412,\n       1.00011418, 0.99989709, 1.00042846, 1.00017823, 1.000166  ,\n       0.99999487, 1.00001276, 1.00004338, 1.00025428, 1.00018777,\n       1.00017304, 1.00049776, 1.00043937, 1.00039794, 1.00040527,\n       1.00001299, 0.99973382, 1.00016141, 1.00029284, 1.00018369,\n       1.00029484, 1.00049281])Indexes: (1)__obs__PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       587, 588, 589, 590, 591, 592, 593, 594, 595, 596],\n      dtype='int64', name='__obs__', length=597))Attributes: (0)\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, saving the best until last - we plot the predictions of the model versus the data and compare to the paper to make sure we are on the right track.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking good, particularly for the partially sheltered location. But we are not matching the paper predictions and the keen eyed reader will understand why. We fit the model giving it no information about which data points belonged to different locations. Our data have heterogeneity - different responses to the same storm event at different locations. What we‚Äôve fit is one parameter to rule them all (per variable). Blended all the locations into one smoothie.\nBut don‚Äôt worry I‚Äôve got you, dawg. We set the data up at the start to make it really easy to build model complexity for just this scenario (whilst keeping the ability to extend into hierarchical models later on)."
  },
  {
    "objectID": "posts/blr/stormerosion_bayesian_linear_regression.html#a-dash-more-complexity",
    "href": "posts/blr/stormerosion_bayesian_linear_regression.html#a-dash-more-complexity",
    "title": "Fitting a Bayesian linear regression for sandy beach storm response",
    "section": "3.2 A dash more complexity",
    "text": "3.2 A dash more complexity\nAs a next step, we could consider that the three locations respond in the same way to storms, but have a different average response. For example, we may think that the relationship between \\(E_{o,cum}\\) and shoreline change, will be strongly positive (XX) at all locations. But that our predicted shoreline change for an average storm would be different when looking at a part of the beach that is sheltered/partially sheltered/exposed.\nThis different mean response we can call a group level intercept, i.e., a different intercept for each of the three locations which form the different groups in our dataset.\n\n# fit the group level intercept location smoothie model\nmod_glint_smoothie = bmb.Model(\n1        \"dW ~ 0 + location + Eocum + Wpre + Dpo + Tpeak + WLres\",\n        df, \n        priors = {\n            \"location\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n            \"Eocum\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n            \"Wpre\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n            \"Dpo\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n            \"Tpeak\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n            \"WLres\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n            \"sigma\": bmb.Prior(\"HalfNormal\", sigma = 2)\n        },\n        family = \"gaussian\"\n    )\n# sampling fairy do your thing\nfit_glint_smoothie = mod_glint_smoothie.fit(\n    draws = 2000\n)\n# and make predictions (added to fit_glint_smoothie)\nmod_glint_smoothie.predict(fit_glint_smoothie, kind = \"response\")\n\n\n1\n\nNote that we have updated the model now to have location (a factor) as a variable. This will create one intercept per location. We also add 0 + to indicate to the model that we don‚Äôt want an overall intercept term. This would be unidentfiable as it could equally have its own value or be abosrbed into the location intercepts.\n\n\n\n\nLets see the difference to our old ‚Äúsmoothie‚Äù model where we simply blended the locations together.\n\n\n\n\n\n\n\n\n\nHey! Nice, you can see from the forest plot that the estimates for each parameter have barely changed, and that we now have those group level intercepts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe‚Äôve lifted our performance a fair bit at the exposed location and a little at the sheltered location. This is nothing to scoff at, we explain a lot of the variability with common regression terms at all locations and a group level intercept. It‚Äôs a nice model, and really this result not unexpected. Beaches erode during storms, to varying degrees depending on the location, but we can expect that the physical drivers of shoreline change have similar effects along a single embayed beach.\nJust like that we have stumbled into the the beginnings of a hierarchical model here, by a loose definition. Some parameters share information from the various groupings in the data and some are informed seperately per group. We can get a little bit smarter about how we do this, in time‚Ä¶\nFor now though, let‚Äôs do this model justice and return to fitting the way the authors intended. Only Bayesian this time. There are three locations and, as the authors identified, there are small but important differences in the response behaviours at these locations.\n\n\n\n\n\n\nCoarse sand graining\n\n\n\n\n\nAt least this is true at the level at which we are describing our processes. There is no difference in the fundamental physics at each location. Quarks be quarks and do quark stuff no matter where they are found on the beach. And perhaps physics based models might resolve the processes down to a level that can replicate the different observed behaviours without resorting to tuning parameter values separately at each location (though IMHO this has been too hard to achieve with any of these sorts of models that I‚Äôve worked with). Definitely though at the coarse grained level we are describing the processes at with our simple linear model, we expect the parameter fits to be different at each location to describe the different magnitudes of response to the same offshore storms.\n\n\n\nAnd so we bestow upon each location its own \\(\\beta\\)s for each covariate. Luckily our data are in a format that will make this very easy as we started to do above. If we specify to the model that it needs three parameters for each \\(\\beta\\), we can use the corresponding \\(\\beta\\)s for each data point depending on its location. We tell this to Bambi with the following formula:\n\n# fit the model separate parameters per location\nmod_separate = bmb.Model(\n1    \"dW ~ 0 + location + Eocum:location + Wpre:location +\" +\n    \"Dpo:location + Tpeak:location + WLres:location\",\n    df,\n    priors = {\n        \"location\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n        \"Eocum\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n        \"Wpre\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n        \"Dpo\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n        \"Tpeak\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n        \"WLres\": bmb.Prior(\"Normal\", mu = 0, sigma = 1),\n        \"sigma\": bmb.Prior(\"HalfNormal\", sigma = 2)\n    },\n    family = \"gaussian\"\n)\n\n\n1\n\nWe are now specifying each variable to have one parameter per location with the :location syntax. If you don‚Äôt believe me, look below to the forest plots and beg forgiveness for your ever having doubted me.\n\n\n\n\nTo be clear, the model we are now fitting could be put into our above notation. We are saying for each observation \\(i\\) we select the appropriate \\(\\beta\\) value based on the corresponding location \\(loc_{[i]}\\):\n\\[\n\\begin{aligned}\n\\Delta W_{[i]} &= \\beta_{0,loc_{[i]}} + \\beta_{1,loc_{[i]}} E_{o,cum[i]} + \\beta_{2,loc_{[i]}} W_{pre[i]} +\n\\\\\n&\\beta_{3,loc_{[i]}} D_{po[i]} + \\beta_{4,loc_{[i]}} T_{p,peak[i]} + \\beta_{5,loc_{[i]}} WL_{res[i]} + \\epsilon\n\\end{aligned}\n\\]\nLets look at our beautiful posterior distributions.\n\n\n\n\n\n\n\n\n\nI really like these plots, we see how the locations differ from each other in small but important ways. We can see locations at which we are more certain/uncertain of the effect of each covariate. We also see how these estimates pool together into a single \\(\\beta\\) for our ‚Äúsmoothie‚Äù model.\nOnce more we plot the predictions and compare back to the paper to check that we recovered similar goodness of fit."
  }
]